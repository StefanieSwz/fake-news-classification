# General Setup

## Clone Repository
Clone the GitHub repository to your local folders:

`git clone https://github.com/StefanieSwz/fake-news-classification.git`

To do so you need access permission, please send an e-mail at st_schwarz@outlook.de.

## Create conda environment
We used anaconda for managing our dependencies. The list of dependencies was partly auto-generated by `pipreqs` for packages that are imported explicetly. Other implecitly called dependencies are added manually to the `requirements.txt`.
To get a complete copy of our development environment, one would have to run the following commands:
```bash
conda create -n mlops python=3.11 # OR: make create_environment
conda activate mlops # OR: conda activate fake-news-classification
pip install -r requirements.txt
pip install nvgpu
pip install -e .

```

## Get data

### Download from kaggle
Since the data is not stored in our GitHub repository, the data needs to be downloaded from Kaggle with the command `make data`. This automatically downloads the dataset and saves the zipped and unzipped data to `data/raw/`.

To preprocess the data type `make preprocess` into your command line. This preprocesses the data and saves the result into `data/processed/`.

### Download from GCS
Another option is to download the current data version from GCS with `dvc pull --force`.

## File structure with Cookiecutter
The file structure was made with the Data Science template probided by Cookiecutter *(no need to repeat again)*, i.e.
```bash
ccds # Set all options and go to created folder
git init
git add .
git commit -m "Initial commit"
git remote add origin <GitHub repo URL>
git branch -M main
git push -u origin main
```
