{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLOps Fake News Classification documentation!","text":""},{"location":"#description","title":"Description","text":"<p>MLOps Project of Tobias Brock, Anne Gritto and Stefanie Schwarz. This project is part of the fullfilment for the lecture \"ML Operations\" from LMU and covers the whole deployment lifecycle of a deep learning project. Here we deploy a model for fake news classification.</p>"},{"location":"#overall-goal-of-the-project","title":"Overall Goal of the Project","text":"<p>The Fake News Classifier project aims to equip students in the ML Operations (MLOps) lecture with hands-on experience in deployment and development techniques. The task of fake news classification is chosen since it offers a straightforward implementation using transformer-based libraries. The goal is to create a robust machine learning model that accurately distinguishes between real and fake news articles.</p>"},{"location":"#framework-and-integration","title":"Framework and Integration","text":"<p>For the classification task we use PyTorch in combination with Hugging Face's Transformers library. PyTorch is a popular deep learning framework known for its flexibility and ease of use. The Transformers library provides pre-trained models and APIs for implementing transformer architectures like BERT (Bidirectional Encoder Representations from Transformers) and SBERT (Sentence-BERT), which are effective for Natural Language Processing (NLP) tasks.</p> <p>To integrate these frameworks, a conda environment is used to manage dependencies and ensure compatibility. Necessary packages like PyTorch, Transformers, and others are installed using pip. The project structure includes scripts for data preprocessing, model training, and evaluation.</p>"},{"location":"#data","title":"Data","text":"<p>For the project, the WELFake dataset from Kaggle is used. This dataset contains 72,134 news articles, with 35,028 labeled as real and 37,106 as fake. It combines four popular news datasets (Kaggle, McIntire, Reuters, BuzzFeed Political) to prevent over-fitting and provide a larger corpus for training. The dataset includes:</p> <ul> <li> <p>Serial Number: A unique identifier for each entry.</p> </li> <li> <p>Title: The headline of the news article.</p> </li> <li> <p>Text: The content of the news article.</p> </li> <li> <p>Label: A binary label indicating whether the news is fake (0) or real (1).</p> </li> </ul>"},{"location":"#models","title":"Models","text":"<p>Suitable models are mainly transformer-based:</p> <ul> <li> <p>Distilled version of BERT without fine-tuning on the training data could serve as baseline model.</p> </li> <li> <p>BERT is pre-trained on a large corpus and can further be fine-tune the model for our specific tasks of fake news classification. Its contextual embeddings will help improve the classifier's accuracy in distinguishing real and fake news.</p> </li> <li> <p>SBERT is a variant of BERT designed to produce semantically meaningful sentence embeddings, making it suitable for understanding sentence-level semantics in news articles.</p> </li> </ul> <p>We decided to focus only on BERT, since the project is meant to showcast the whole deployment cycle of a deep learning model and supporting different model classes would increase the amount of backend code tremendously.</p>"},{"location":"#overview","title":"Overview","text":"Architecture overview"},{"location":"ci/","title":"Continuous Integration","text":""},{"location":"ci/#unit-tests","title":"Unit tests","text":"<p>Unit testing refers to the practice of writing test that tests individual parts of our code base to test for correctness. We use <code>Pytest</code> to test our code base. To locally check if our tests are passing, we type in a terminal <code>pytest tests/</code>. We measure the amount of code our tests cover with the package <code>coverage</code>. Then, we adapt the prompt to <code>coverage run -m pytest tests/</code>. To get a simple coverage report, type <code>coverage report</code> which will give us the percentage of cover in each of our files. By writing <code>coverage report -m</code>, we get the exact lines that were missed by our tests.</p> <pre><code>pytest tests/       # check if tests are passing\ncoverage report     # simple coverage report\ncoverage report -m  # exact lines missed by tests\n</code></pre>"},{"location":"ci/#github-actions","title":"GitHub actions","text":"<p>Each repository gets 2000 minutes of free testing per month. It is added in <code>.github/workflows/tests.yaml</code> which should automatically run the tests for us when pushin to main or to a branch with PR to main. It works by initiating a Python environmen, installing all dependencies and then <code>pytest</code> is called to run the tests. We execute our tests on different operating systems, i.e. Linux, Windows and Mac. Our minutes for July are exhausted, therefore our tests fail right now when pushing on main, but work locally.</p>"},{"location":"ci/#code-style","title":"Code style","text":"<p>We use <code>ruff</code> to check and re-format our code. The file <code>.github/workflows/codecheck.yaml</code> that also sets up a Python environment, installs the requirements and runs <code>ruff check .</code> and <code>ruff format .</code> which checks and formats the code according to certain Pep8 style guidelines, e.g. trimming white spacing.</p>"},{"location":"ci/#pre-commit","title":"Pre-commit","text":"<p>Pre-commits can help us attach additional tasks that should be run every time that we do a\u00a0<code>git commit</code>. Setup:</p> <pre><code>pip install pre-commit\npre-commit sample-config &gt; .pre-commit-config.yaml #(or git pull, if file already exists, important: utf-8)\npre-commit install\npre-commit run --all-files  # fixes all files\ngit commit -m \"Commit message\" --no-verify  # usual commit\n</code></pre> <p>We can either add the modified files to the staging area and commit. If we have style issues for example, the pre-commit fixes those such that we can re-add them, commit and push. Otherwise we could run <code>pre-commit run --all-files</code> which fixes all files. If we want to do usual commits, we add as usual and then type <code>git commit -m \"Commit message\" --no-verify</code>.</p>"},{"location":"docker/","title":"Docker","text":"<p>Make sure to have Docker Desktop installed. The data also needs to be accessed, either with your local data folder or via the data bucket in GCS.</p>"},{"location":"docker/#about-docker","title":"About Docker","text":"<p>A Docker image is a template for a Docker container. A Docker container is a running instance of a Docker image. A Docker image is a static file, while a Docker container is a running process. Running inside a Docker container gives you a consistent and independent environment for your application. This means that you can be sure that your application will run the same way on your machine as it will on another machine. Thus, Docker gives the ability to abstract away the differences between different machines.</p>"},{"location":"docker/#general-useful-commands","title":"General useful commands","text":"<pre><code>docker images               # list all images\ndocker rmi &lt;image_id&gt;       # remove a docker image\ndocker run                  # create a docker container\ndocker start &lt;container_id&gt; # restart docker container\ndocker stop &lt;container_id&gt;  # stop docker container\ndocker ps                   # list containers\ndocker rm &lt;container_id&gt;    # remove docker container\n</code></pre>"},{"location":"docker/#local-docker","title":"Local Docker","text":"<p>To run a dockerfile locally, for example to train the model, type</p> <pre><code># Training\ndocker build -f docker/trainer_local.dockerfile . -t trainer:latest # creates a docker image\n## Create Docker container with mounted models folder\ndocker run --name &lt;container_name&gt; -v %cd%/models:/models/ trainer:latest train.local_data=True train.local_wandb=True &lt;model.dropout_rate=0.3 train.batch_size=8&gt; # Windows command line\ndocker run --name &lt;container_name&gt; -v $(pwd)/models:/models/ trainer:latest # for Powershell/Linux\ndocker run --name &lt;container_name&gt; -v ${PWD}/models:/models/ trainer:latest # if something else does not work\n\n# Predicting\ndocker build -f docker/predict_local.dockerfile . -t predict:latest\ndocker run --name &lt;container_name&gt; predict:latest predict.local_data=True predict.local_wandb=True\n</code></pre>"},{"location":"docker/#cloud-docker","title":"Cloud Docker","text":"<p>Authentification for Docker with GCP: <code>gcloud auth configure-docker europe-west3.pkg.dev</code></p>"},{"location":"docker/#images","title":"Images","text":"<p>We use\u00a0Cloud Build\u00a0for building the containers in the cloud and\u00a0Artifact registry\u00a0for storing the images afterward. The latest image is automatically triggered when pushing code to main. We have 5 images in our Google Cloud Artifact Registry, <code>backend, frontend, monitoring, predict</code> and <code>trainer</code>.</p>"},{"location":"docker/#cloudbuilds","title":"Cloudbuilds","text":"<p>To build containers in the cloud from local, we use the <code>cloudbuild.yaml</code> which in our case builds the docker images of <code>trainer</code> and <code>predict</code> and pushes them to the Artifact registry.</p> <pre><code>gcloud builds submit \u2014-config=config/cloudbuild.yaml .            # for trainer and predict\ngcloud builds submit \u2014-config=config/cloudbuild_backend.yaml .    # builds, pushes and deploys\ngcloud builds submit \u2014-config=config/cloudbuild_frontend.yaml .   # builds, pushes and deploys\ngcloud builds submit \u2014-config=config/cloudbuild_monitoring.yaml . # builds, pushes and deploys\n</code></pre>"},{"location":"docker/#push-und-pull","title":"Push und pull","text":"<p>To push and pull to our Artifact registry, we refer to this URL: <code>europe-west3-docker.pkg.dev/mlops-fakenews/mlops-fakenews-container</code>, e.g. to pull the latest trainer image, type</p> <p><code>docker pull europe-west3-docker.pkg.dev/mlops-fakenews/mlops-fakenews-container/trainer:latest</code></p> <p>Manually creating images &amp; push them, example with busybox:</p> <pre><code>docker tag busybox &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;/busybox:latest\ndocker push &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;/busybox:latest\n</code></pre>"},{"location":"getting-started/","title":"Getting started","text":"<p>This is where you describe how to get set up on a clean install, including the commands necessary to get the raw data (using the <code>sync_data_from_s3</code> command, for example), and then how to make the cleaned, final data sets.</p>"},{"location":"run_local/","title":"Model training","text":"<p>We can train the model locally or in the Cloud. By default, if a model has a lower validation loss than the best model stored in GCS, it will be stored in GCS as best model. If a model should not be automatically be replaced by a newly trained model, set <code>cfg.cloud.save_best_model_gcs=False</code>.</p>"},{"location":"run_local/#local-training","title":"Local training","text":"<p>To train from local, we can make use of our Makefile and change the default hydra configuration as well. Training with defaults can be run with <code>make train</code>. Models are by default logged to wandb but not automatically saved to a local folder. Logging and model saving can be deactivated during training with the following commands:</p> <p><code>make train ARGS=\"train.save_model=False train.log_model=False\u201d</code></p> <p>The default parameters in the <code>train.yaml</code>are set to the following:</p> <pre><code>lr: 1e-5\nbatch_size: 32\nepochs: 1\npatience: 3\nrandom_state: 2018\ntest_size: 0.3\nval_size: 0.5\nfilename: 'model'\nverbose: True\ndevices: 1\nlog_every_n_steps: 50\nrefresh_rate: 50\nprecision: \"32\"\nprofiler: \"simple\"\nnum_runs: 5\nsweep: False\nsave_model: False\nlog_model: True\nlocal_wandb: False\nlocal_data: False\n</code></pre> <p>To train with hyperparameter optimization, sweep can be run with <code>make train ARGS=train.sweep=True</code>.</p>"},{"location":"run_local/#training-on-google-cloud","title":"Training on Google Cloud","text":"<p>We used two different services to train on the Cloud, Vertex AI and the Compute Engine. We were able to increase quota for both services such that we can train on a Nvidia T4 GPU as well as on CPUs only.</p>"},{"location":"run_local/#vertex-ai","title":"Vertex AI","text":"<ol> <li>Check that <code>config_cpu.yaml</code> and/or <code>config_gpu.yaml</code> are in <code>config/</code>. <code>config_cpu.yaml</code> specifies the machine type we are using (<code>n1-highnem-2</code>) and the docker image saved in the Artifact registry <code>europe-west3-docker.pkg.dev/mlops-fakenews/mlops-fakenews-container/trainer:latest</code>. <code>config_gpu.yaml</code> uses the same image but as machine type <code>n1-standard-8</code> and specifies additionally the accelerator type as <code>NVIDIA_TESLA_T4</code>.</li> <li>Train in the cloud on a CPU by <code>gcloud ai custom-jobs create --region=europe-west3 --display-name=train-run --config=config/config_cpu.yaml --service-account=sa-mlops@mlops-fakenews.iam.gserviceaccount.com</code> optionally specify parameters: <code>--args train.epochs=1</code></li> </ol> <p>Train in the cloud on a GPU by <code>gcloud ai custom-jobs create --region=europe-west1 --display-name=test-run-gpu-3 --config=config/config_gpu.yaml --service-account=sa-mlops@mlops-fakenews.iam.gserviceaccount.com</code>. Note that setting the right region (<code>europe-west1</code>) is important to access the GPU.</p> <ol> <li>Monitor on GCP: <code>Vertex AI &gt; Training &gt; Custom Jobs &gt; View Logs</code>. Also here navigate to the region you specified in the gcloud command.</li> </ol>"},{"location":"run_local/#compute-engine","title":"Compute Engine","text":"<p>To train on a GPU, we selected an image with Nvidia drivers and Pytorch (<code>c0-deeplearning-common-cu121-v20240627-debian-11-py310</code>) to create a Virtual Machine with 100GB storage and <code>n1-standard-8</code> machine type. When starting The VM for the first time you are prompted: <code>install nvidia driver?[y/n]</code> and then type <code>y</code>. Git is already installed, <code>nvidia-smi</code> and <code>conda</code>are available.</p> <p>The rest of the training is analoug to local training.</p>"},{"location":"setup/","title":"General Setup","text":""},{"location":"setup/#clone-repository","title":"Clone Repository","text":"<p>Clone the GitHub repository to your local folders:</p> <p><code>git clone https://github.com/StefanieSwz/fake-news-classification.git</code></p> <p>To do so you need access permission, please send an e-mail at st_schwarz@outlook.de.</p>"},{"location":"setup/#create-conda-environment","title":"Create conda environment","text":"<p>We used anaconda for managing our dependencies. The list of dependencies was partly auto-generated by <code>pipreqs</code> for packages that are imported explicetly. Other implecitly called dependencies are added manually to the <code>requirements.txt</code>. To get a complete copy of our development environment, one would have to run the following commands:</p> <pre><code>conda create -n mlops python=3.11 # OR: make create_environment\nconda activate mlops # OR: conda activate fake-news-classification\npip install -r requirements.txt\npip install nvgpu\npip install -e .\n\n</code></pre>"},{"location":"setup/#get-data","title":"Get data","text":""},{"location":"setup/#download-from-kaggle","title":"Download from kaggle","text":"<p>Since the data is not stored in our GitHub repository, the data needs to be downloaded from Kaggle with the command <code>make data</code>. This automatically downloads the dataset and saves the zipped and unzipped data to <code>data/raw/</code>.</p> <p>To preprocess the data type <code>make preprocess</code> into your command line. This preprocesses the data and saves the result into <code>data/processed/</code>.</p>"},{"location":"setup/#download-from-gcs","title":"Download from GCS","text":"<p>Another option is to download the current data version from GCS with <code>dvc pull --force</code>.</p>"},{"location":"setup/#file-structure-with-cookiecutter","title":"File structure with Cookiecutter","text":"<p>The file structure was made with the Data Science template probided by Cookiecutter (no need to repeat again), i.e.</p> <pre><code>ccds # Set all options and go to created folder\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit remote add origin &lt;GitHub repo URL&gt;\ngit branch -M main\ngit push -u origin main\n</code></pre>"},{"location":"setup/#cloud-setup","title":"Cloud Setup","text":"<ol> <li>Make sure that Google Cloud CLI is installed and initialized by <code>gcloud init</code>.</li> <li>Authenticate account with the following command: <code>gcloud auth login</code></li> <li>Configure authentification with service account credentials by <code>gcloud auth activate-service-account sa-mlops@mlops-fakenews.iam.gserviceaccount.com --key-file=service_account_credentials.json</code></li> </ol>"}]}